{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "parquet_path = \"data/merged_output_parquet/*.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512c31e",
   "metadata": {},
   "source": [
    "First we test that we can read our parquet files with DuckDB by checking that all columns are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5445da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.read_parquet(f\"{parquet_path}\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77c60c",
   "metadata": {},
   "source": [
    "We do some initial data anlysis by calculating the Average, Minimum, and Maximum values for the most interesting columns.\n",
    "\n",
    "Also we calculate the speed of execution se we can later compare it to Dask SQL.\n",
    "\n",
    "We see that we should do some filtering because minimum for trip_distance and total_amount is negative, which are not results that should be possible. Also we should cap the total_amount because we doubt that anyone would pay 1000003.8 for a taxi ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc60b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "duckdb.sql(f\"\"\" select \n",
    "                avg(passenger_count) avg_passengers, \n",
    "                avg(trip_distance),\n",
    "                min(trip_distance),\n",
    "                max(trip_distance),\n",
    "                avg(total_amount),\n",
    "                min(total_amount),\n",
    "                max(total_amount) \n",
    "           from '{parquet_path}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cfa14",
   "metadata": {},
   "source": [
    "Calculate the median trip distance of rides that were done for each day of the year, we separete the results by month so we can visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ae73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = duckdb.sql(f\"\"\" select month(tpep_pickup_datetime), day(tpep_pickup_datetime), round(mean(trip_distance), 2) as avg_distance from  \n",
    "           '{parquet_path}'\n",
    "           group by day(tpep_pickup_datetime), month(tpep_pickup_datetime) \n",
    "           order by month(tpep_pickup_datetime), day(tpep_pickup_datetime)\n",
    "\"\"\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf04734",
   "metadata": {},
   "source": [
    "We are going to visualize the results we have calculated earlier, so we get a better understanding for the trips taken during the year. As we can see, for the first four months of the year, people are usually taking shorter trips compared to the other parts of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d83e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, columns=['month', 'day', 'avg_distance'])\n",
    "df = df.sort_values(by=['month', 'day'])\n",
    "df['date'] = df.apply(lambda row: str(int(row['day'])) + \"-\" + str(int(row['month'])), axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "plt.bar(df['date'], df['avg_distance'], label='Daily Avg Distance')\n",
    "\n",
    "plt.title('Average Trip Distance Over the Year', fontsize=18)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Average Trip Distance (miles)', fontsize=14) \n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "ax = plt.gca()\n",
    "# ax.xaxis.set_major_locator(ticker.MaxNLocator(12))\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(df[df['day'] == 1].index))\n",
    "# ax.axis.set_major_locator( df[df['day'] == 1]['date']) \n",
    "\n",
    "# Adding a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout() # Adjusts plot to ensure everything fits without overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30172b3",
   "metadata": {},
   "source": [
    "Next we take a look at the median fare amount during different hours of the day. As we can see the taxi drivers earn the most in the early morning, during 4, 5, 6, hours in the morning. Also there are some inconsistencies which should be looked at, we have a negative median fare amount for the 8th hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d765a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result =  duckdb.sql(f\"\"\" select hour(tpep_pickup_datetime), round(mean(fare_amount), 2) as avg_amount from  \n",
    "           '{parquet_path}'\n",
    "           group by hour(tpep_pickup_datetime)\n",
    "           order by hour(tpep_pickup_datetime)\n",
    "\"\"\").fetchall()\n",
    "\n",
    "df = pd.DataFrame(result, columns=['hour', 'fare_amount'])\n",
    "df = df.sort_values(by=['hour'])\n",
    "\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "plt.bar(df['hour'], df['fare_amount'], label='Hourly Average Fare Amount')\n",
    "\n",
    "plt.title('Hourly Average Fare Amount', fontsize=18)\n",
    "plt.xlabel('Hour', fontsize=14)\n",
    "plt.ylabel('Average Fare Amount', fontsize=14) \n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "ax = plt.gca()\n",
    "# ax.xaxis.set_major_locator(ticker.MaxNLocator(12))\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator(df['hour'].index))\n",
    "# ax.axis.set_major_locator( df[df['day'] == 1]['date']) \n",
    "\n",
    "# Adding a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout() # Adjusts plot to ensure everything fits without overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34656997",
   "metadata": {},
   "source": [
    "We take a look at the borughs that people usually take trips between. And also calculate the time needed to calculate this query in order to compare it to the Dask SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "duckdb.sql(f\"\"\" select borough_pickup, borough_dropoff, count(*) as trips_count from  \n",
    "           '{parquet_path}'\n",
    "           where borough_pickup is not null and borough_dropoff is not null\n",
    "           group by borough_pickup, borough_dropoff\n",
    "           order by trips_count desc\n",
    "\"\"\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb840e",
   "metadata": {},
   "source": [
    "In the following cells, we try to make some conclusion based on the data we have added to the original data. We take the look at the fact if the day is rainy or if it is a holiday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\" select is_holiday, round(avg(trip_distance), 2), round(mean(tip_amount), 2) median_tip from  \n",
    "           '{parquet_path}'\n",
    "           group by is_holiday \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbab726",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\" select round(avg(no_trips)) avg_trips_holiday from\n",
    "           (\n",
    "                select year(tpep_pickup_datetime), month(tpep_pickup_datetime), day(tpep_pickup_datetime), count(*) as no_trips from  \n",
    "                '{parquet_path}'\n",
    "                where is_holiday is true\n",
    "                group by year(tpep_pickup_datetime), month(tpep_pickup_datetime), day(tpep_pickup_datetime)\n",
    "           )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ddcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\" select round(avg(no_trips)) as avg_trips_non_holiday from\n",
    "           (\n",
    "                select year(tpep_pickup_datetime), month(tpep_pickup_datetime), day(tpep_pickup_datetime), round(count(*)) as no_trips from  \n",
    "                '{parquet_path}'\n",
    "                where is_holiday is false\n",
    "                group by year(tpep_pickup_datetime), month(tpep_pickup_datetime), day(tpep_pickup_datetime)\n",
    "           )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bca887",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\" select round(avg(trip_distance), 2), round(mean(tip_amount), 2) median_tip from  \n",
    "           '{parquet_path}'\n",
    "           where \"rain (mm)\" >= 0.5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\" select round(avg(trip_distance), 2), round(mean(tip_amount), 2) median_tip from  \n",
    "           '{parquet_path}'\n",
    "           where \"rain (mm)\" < 0.5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd418310",
   "metadata": {},
   "source": [
    "We are creating an Dask client in order to compute the queries using the Dask SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, \n",
    "from dask_sql import Context\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='8GB')\n",
    "print(f\"Dask Dashboard link: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"data/merged_output_parquet\" \n",
    "c = Context()\n",
    "\n",
    "\n",
    "dask_table_name = \"taxi_data\"\n",
    "c.create_table(dask_table_name, parquet_path, format=\"parquet\")\n",
    "\n",
    "\n",
    "sql_query = f\"\"\" select borough_pickup, borough_dropoff, count(*) as trips_count from  \n",
    "           '{dask_table_name}'\n",
    "           where borough_pickup is not null and borough_dropoff is not null\n",
    "           group by borough_pickup, borough_dropoff\n",
    "           order by trips_count desc\n",
    "\"\"\"\n",
    "\n",
    "sql_query_initial = f\"\"\"\n",
    "    select \n",
    "                avg(passenger_count) avg_passengers, \n",
    "                avg(trip_distance),\n",
    "                min(trip_distance),\n",
    "                max(trip_distance),\n",
    "                avg(total_amount),\n",
    "                min(total_amount),\n",
    "                max(total_amount) \n",
    "           from '{dask_table_name}'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04106bde",
   "metadata": {},
   "source": [
    "For this initial query we can see that by using DuckDB we can save a lot of time compared to Dask SQL implementation. \n",
    "For my local implementation I can notice that using DuckDB needs 2.11 ms to compute this query, where as Dask SQL requires 6.48 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "c.sql(sql_query_initial).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20b324",
   "metadata": {},
   "source": [
    "The same thing can be noticed in this example here, DuckDB requires significently less time to calculate the same query comapred to Dask SQL. \n",
    "We can see that the Dask SQL locally takes 46.4 s to compute this query, while DuckDB need only 669 ms, which is more than 50 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c.sql(sql_query).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0b42d-2740-4033-8a4d-dee6cc0e45bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "976ad147-fb3f-426d-9d96-110937aff48c",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f35b7a-8791-467e-84dd-a207e3ed072d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 104.61 ms\n",
      "[########################################] | 100% Completed | 2.35 sms\n",
      "[########################################] | 100% Completed | 204.93 ms\n",
      "[########################################] | 100% Completed | 205.54 ms\n",
      "[########################################] | 100% Completed | 2.24 ss\n",
      "[########################################] | 100% Completed | 101.79 ms\n",
      "[########################################] | 100% Completed | 105.82 ms\n",
      "[########################################] | 100% Completed | 624.05 ms\n",
      "[########################################] | 100% Completed | 5.90 ss\n",
      "[########################################] | 100% Completed | 7.60 ss\n",
      "[########################################] | 100% Completed | 10.14 s\n",
      "[########################################] | 100% Completed | 310.92 ms\n",
      "[########################################] | 100% Completed | 2.26 ss\n",
      "[########################################] | 100% Completed | 102.91 ms\n",
      "[########################################] | 100% Completed | 102.22 ms\n",
      "[########################################] | 100% Completed | 650.53 ms\n",
      "\n",
      "Wall‑time (seconds) on three sample files:\n",
      "\n",
      "query                Q1     Q2      Q3\n",
      "engine   fmt                          \n",
      "dask     csv      5.903  7.628  10.169\n",
      "         parquet  0.108  2.362   0.224\n",
      "dask-sql csv      0.335  2.487   0.688\n",
      "         parquet  0.214  2.464   0.666\n",
      "duckdb   csv      1.039  1.035   1.029\n",
      "         parquet  0.014  0.034   0.029\n",
      "pandas   csv      0.007  0.670   0.370\n",
      "         parquet  0.012  0.940   0.387\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, time, glob, contextlib, textwrap\n",
    "import duckdb, pandas as pd, dask.dataframe as dd, dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask_sql import Context\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# paths\n",
    "# ------------------------------------------------------------------\n",
    "BASE          = \"/Users/amadej/Desktop/big_data/assignment5/big-data-project/data/sample_merged_output\"\n",
    "PARQUET_GLOB  = f\"{BASE}/part*.parquet\"\n",
    "PARQUET       = sorted(glob.glob(PARQUET_GLOB))\n",
    "\n",
    "CSV_DIR       = f\"{BASE}/csv_cache\"\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "CSV_GLOB      = f\"{CSV_DIR}/part*.csv\"\n",
    "\n",
    "# one‑time parquet to csv export (so benchmarking is fair)\n",
    "for p in PARQUET:\n",
    "    stem = os.path.basename(p)[:-8]\n",
    "    out  = f\"{CSV_DIR}/{stem}.csv\"\n",
    "    if not os.path.exists(out):\n",
    "        pd.read_parquet(p).to_csv(out, index=False)\n",
    "CSV = sorted(glob.glob(CSV_GLOB))\n",
    "\n",
    "# columns actually used by the three test queries\n",
    "USECOLS = [\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"trip_distance\",\n",
    "    \"tip_amount\",\n",
    "    \"borough_pickup\",\n",
    "    \"borough_dropoff\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# helper utilities\n",
    "# ------------------------------------------------------------------\n",
    "results = []                       # wall‑times end up here\n",
    "def timer(label, qid, fmt):\n",
    "    \"\"\"context‑manager that appends duration to results list\"\"\"\n",
    "    @contextlib.contextmanager\n",
    "    def _t():\n",
    "        t0 = time.perf_counter();  yield\n",
    "        results.append(dict(engine=label, query=qid, fmt=fmt,\n",
    "                            sec=time.perf_counter() - t0))\n",
    "    return _t()\n",
    "\n",
    "# three toy analytics\n",
    "def q1(df):               # average trip distance\n",
    "    return df[\"trip_distance\"].mean()\n",
    "\n",
    "def q2(df):               # 10 busiest borough OD pairs\n",
    "    return (\n",
    "        df.groupby([\"borough_pickup\", \"borough_dropoff\"])\n",
    "          .size()\n",
    "          .nlargest(10)\n",
    "    )\n",
    "\n",
    "def q3(df):               # mean tip by hour‑of‑day\n",
    "    df2 = df.assign(hour = dd.to_datetime(df[\"tpep_pickup_datetime\"]).dt.hour\n",
    "                    if isinstance(df, dd.DataFrame)\n",
    "                    else pd.to_datetime(df[\"tpep_pickup_datetime\"]).dt.hour)\n",
    "    return df2.groupby(\"hour\")[\"tip_amount\"].mean()\n",
    "\n",
    "QUERIES = {\"Q1\": q1, \"Q2\": q2, \"Q3\": q3}\n",
    "SQL = {\n",
    "    \"Q1\": \"SELECT AVG(trip_distance) FROM df\",\n",
    "    \"Q2\": textwrap.dedent(\"\"\"\n",
    "         SELECT borough_pickup, borough_dropoff, COUNT(*) trips\n",
    "         FROM df\n",
    "         GROUP BY 1,2\n",
    "         ORDER BY trips DESC\n",
    "         LIMIT 10\n",
    "    \"\"\"),\n",
    "    \"Q3\": textwrap.dedent(\"\"\"\n",
    "         WITH tmp AS (\n",
    "           SELECT\n",
    "             date_part('hour', CAST(tpep_pickup_datetime AS TIMESTAMP)) AS hr,\n",
    "             tip_amount\n",
    "           FROM df\n",
    "         )\n",
    "         SELECT\n",
    "           hr,\n",
    "           AVG(tip_amount) AS avg_tip\n",
    "         FROM tmp\n",
    "         GROUP BY hr\n",
    "    \"\"\"),\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# global Dask memory limits & spilling\n",
    "# ------------------------------------------------------------------\n",
    "dask.config.set({\n",
    "    \"distributed.worker.memory.target\": 0.60,\n",
    "    \"distributed.worker.memory.spill\" : 0.70,\n",
    "    \"distributed.worker.memory.pause\" : 0.85,\n",
    "})\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# benchmark loop parquet vs csv\n",
    "# ------------------------------------------------------------------\n",
    "for fmt, paths, glob_pat in [\n",
    "        (\"parquet\", PARQUET, PARQUET_GLOB),\n",
    "        (\"csv\",     CSV,     CSV_GLOB)]:\n",
    "\n",
    "    # ------------- DuckDB ---------------------------------------------------\n",
    "    con = duckdb.connect()\n",
    "    con.execute(f\"CREATE OR REPLACE VIEW df AS SELECT * FROM read_{fmt}('{glob_pat}')\")\n",
    "    for qid in QUERIES:\n",
    "        with timer(\"duckdb\", qid, fmt):\n",
    "            con.execute(SQL[qid]).fetchall()\n",
    "\n",
    "    # ------------- Pandas ----------------------------------------------------\n",
    "    if fmt == \"parquet\":\n",
    "        pdf = pd.concat([pd.read_parquet(p, columns=USECOLS) for p in paths])\n",
    "    else:\n",
    "        pdf = pd.concat([\n",
    "            pd.read_csv(\n",
    "                p,\n",
    "                low_memory=False,\n",
    "                usecols=USECOLS,\n",
    "                parse_dates=[\"tpep_pickup_datetime\"],\n",
    "                dtype={\"borough_pickup\":\"string\",\n",
    "                       \"borough_dropoff\":\"string\"},\n",
    "            ) for p in paths\n",
    "        ])\n",
    "    for qid, fn in QUERIES.items():\n",
    "        with timer(\"pandas\", qid, fmt):\n",
    "            fn(pdf)\n",
    "\n",
    "    # ------------- Dask DataFrame -------------------------------------------\n",
    "    if fmt == \"parquet\":\n",
    "        ddf = dd.read_parquet(\n",
    "            paths,\n",
    "            columns=USECOLS,\n",
    "            gather_statistics=False,\n",
    "            blocksize=\"16MB\",\n",
    "            split_row_groups=True,\n",
    "            engine=\"pyarrow\",\n",
    "        )\n",
    "    else:\n",
    "        # tell Dask that tpep_pickup_datetime is datetime64\n",
    "        ddf = dd.read_csv(\n",
    "            paths,\n",
    "            usecols=USECOLS,\n",
    "            assume_missing=True,\n",
    "            dtype_backend=\"pyarrow\",\n",
    "            blocksize=\"16MB\",\n",
    "        )\n",
    "\n",
    "    ddf[\"tpep_pickup_datetime\"] = dd.to_datetime(ddf[\"tpep_pickup_datetime\"])\n",
    "\n",
    "    for qid, fn in QUERIES.items():\n",
    "        with ProgressBar(), timer(\"dask\", qid, fmt):\n",
    "            fn(ddf).compute()\n",
    "\n",
    "    # ------------- Dask‑SQL --------------------------------------------------\n",
    "    ctx = Context()\n",
    "    ctx.create_table(\"df\", PARQUET_GLOB)   # <-- a single string\n",
    "    for qid in QUERIES:\n",
    "        with ProgressBar(), timer(\"dask-sql\", qid, fmt):\n",
    "            ctx.sql(SQL[qid]).compute()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# pretty print\n",
    "# ------------------------------------------------------------------\n",
    "tbl = pd.DataFrame(results).pivot_table(\n",
    "        index=[\"engine\", \"fmt\"], columns=\"query\", values=\"sec\")\n",
    "print(\"\\nWall‑time (seconds) on three sample files:\\n\")\n",
    "print(tbl.round(3))\n",
    "\n",
    "\n",
    "# Summary:\n",
    "#\n",
    "# Wall-time (seconds):\n",
    "#                Q1      Q2      Q3\n",
    "# dask     csv   4.922   5.194   7.056\n",
    "#          parquet 0.110   2.336   0.230\n",
    "# dask-sql csv   0.241   2.591   0.681\n",
    "#          parquet 0.234   2.832   0.525\n",
    "# duckdb   csv   1.076   1.123   1.138\n",
    "#          parquet 0.012   0.035   0.041\n",
    "# pandas   csv   0.007   0.673   0.378\n",
    "#          parquet 0.015   1.034   0.655\n",
    "#\n",
    "# Key takeaways:\n",
    "# 1) Parquet versus CSV:\n",
    "#    - Dask and DuckDB see huge wins with Parquet over CSV.\n",
    "#    - Pandas still reads CSV slightly faster for simple scans, but group-bys tip the balance toward Parquet.\n",
    "#    - Dask-SQL sits between: it benefits from Parquet but has extra SQL-layer cost.\n",
    "#\n",
    "# 2) Comparing engines on Parquet:\n",
    "#    - DuckDB is unbeatable on Parquet (Q1 in 0.012 s, Q2 in 0.035 s, Q3 in 0.041 s).\n",
    "#    - Dask (pure) is next: parallelism cuts Q3 to 0.230 s.\n",
    "#    - Dask-SQL adds flexibility but pays a premium (Q2 takes 2.832 s vs. 2.336 s in raw Dask).\n",
    "#    - Pandas holds its own on simple queries (Q1 0.015 s) but slows on group-bys (Q2 1.034 s).\n",
    "#\n",
    "# 3) Comparing engines on CSV:\n",
    "#    - Pandas excels at Q1 on CSV (0.007 s) and remains competitive for simple group-bys.\n",
    "#    - DuckDB on CSV is consistent (~1.1 s across all queries).\n",
    "#    - Dask-SQL on CSV (Q1 0.241 s) outperforms DuckDB on Q1, but for grouping (Q2/Q3) it’s slower.\n",
    "#    - Pure Dask on CSV is the slowest (5–7 s range), due to parsing and scheduling overhead.\n",
    "#\n",
    "# Conclusion:\n",
    "# - For quick ad-hoc lookups on Parquet, DuckDB is the clear winner.\n",
    "# - For scale-out or larger-than-memory workloads on Parquet, raw Dask is a strong candidate.\n",
    "# - Dask-SQL offers SQL syntax over Dask but with measurable overhead—best when you need that integration.\n",
    "# - Pandas remains great for small CSV analyses, but switch to columnar formats and DuckDB or Dask when queries get heavier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead01813-8384-48a9-8800-8406e854b486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
