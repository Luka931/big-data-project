{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d0a1eb",
   "metadata": {},
   "source": [
    "# Docker \n",
    "\n",
    "In order to set up Kafka and Zookeeper, we are going to utilize docker containers. We just need to run the \"docker-compose up -d\" command, and it is going to create the required containers for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer,  Consumer, KafkaException, KafkaError\n",
    "import json\n",
    "import duckdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38621a02",
   "metadata": {},
   "source": [
    "# Producer\n",
    "\n",
    "Here we defin the producer which is going to publish messages to the 'yellow-taxi' topic. \n",
    "This topic is to be automatically created in our Kafka instance which is running on the local machine, on the port 9092.\n",
    "When we send this message we are only going to make a console log about it delivery, if it fails we notify the user, otherwise print out the topic we are writting to, the partition and the message offset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "YELLOW_TAXI_TOPIC = 'yellow-taxi'\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered to topic '{msg.topic()}' \"\n",
    "              f\"[{msg.partition()}] @ offset {msg.offset()}\")\n",
    "\n",
    "def create_producer():\n",
    "    conf = {\n",
    "        'bootstrap.servers': BOOTSTRAP_SERVERS,\n",
    "        'client.id': 'python-producer'\n",
    "    }\n",
    "    try:\n",
    "        producer = Producer(conf)\n",
    "        print(\"Confluent Kafka Producer initialized successfully.\")\n",
    "        return producer\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Confluent Kafka Producer: {e}\")\n",
    "        return None\n",
    "\n",
    "def send_message(producer, topic_name, message_value):\n",
    "    if producer is None:\n",
    "        print(\"Producer is not initialized. Cannot send message.\")\n",
    "        return\n",
    "    try:\n",
    "        producer.produce(topic_name,\n",
    "                         value=json.dumps(message_value).encode('utf-8'),\n",
    "                         callback=delivery_report)\n",
    "        \n",
    "        producer.poll(0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending message: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ea3a0",
   "metadata": {},
   "source": [
    "## Generator functions\n",
    "\n",
    "We are going to utillize the ducdb library to fetch the data from the parquet files. We decided to do it this way (even though it is not the most optimal), because our files are not sorted by pickup data, and we are low on storage to write it once again to the drive.\n",
    "We fetch the data in batches of 1.000.000 rows, and return it in the Pandas dataframe so it can be later used by the producer.\n",
    "We provided functions both for the High Volume Dataset, and Yellow taxi dataset, because we are going to utilize both for the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_high_volume(batch_size = 1_000_00):\n",
    "    high_volume_path = 'data/trip_record_partitioned/high_volume/year=2021/*.parquet'\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        df = duckdb.sql(f\"\"\"\n",
    "            select \n",
    "                pickup_datetime, \n",
    "                dropoff_datetime, \n",
    "                PULocationID, \n",
    "                DOLocationID, \n",
    "                trip_miles as distance, \n",
    "                base_passenger_fare as base_fare, \n",
    "                congestion_surcharge as congestion_charge, \n",
    "                tips\n",
    "            from '{high_volume_path}'\n",
    "            order by pickup_datetime\n",
    "            limit {batch_size}\n",
    "            offset {offset}\n",
    "        \"\"\").df()\n",
    "        df['pickup_datetime'] = df['pickup_datetime'].astype('str')\n",
    "        df['dropoff_datetime'] = df['dropoff_datetime'].astype('str')\n",
    "\n",
    "        offset = offset + batch_size\n",
    "\n",
    "        yield df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2263c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_yellow_taxi(batch_size = 10_000_000):\n",
    "    yellow_taxi_path = 'data/trip_record_partitioned/yellow-taxi/year=2021/*.parquet'\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        df = duckdb.sql(f'''\n",
    "            select \n",
    "                tpep_pickup_datetime as pickup_datetime,\n",
    "                tpep_dropoff_datetime as dropoff_datetime,\n",
    "                PULocationID,\n",
    "                DOLocationID,\n",
    "                trip_distance as distance,\n",
    "                fare_amount as base_fare,\n",
    "                passenger_count,\n",
    "                payment_type\n",
    "            from '{yellow_taxi_path}'\n",
    "            order by pickup_datetime\n",
    "            limit {batch_size}\n",
    "            offset {offset}\n",
    "        ''').df()\n",
    "        df['pickup_datetime'] = df['pickup_datetime'].astype('str')\n",
    "        df['dropoff_datetime'] = df['dropoff_datetime'].astype('str')\n",
    "\n",
    "\n",
    "        offset = offset + batch_size\n",
    "        yield df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574edb09",
   "metadata": {},
   "source": [
    "### Running the producer\n",
    "\n",
    "We need to initialize our producer, and generator function in order to publish data to the topic. \n",
    "After we initialize them, we take our rows of data batch by batch and publish them to the topic. We do this until we run out of the rows in the dataset. \n",
    "After each abtch we flush the producer so our queue does not get filled up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = get_batch_yellow_taxi()\n",
    "\n",
    "producer = create_producer()\n",
    "\n",
    "if producer:\n",
    "    for records in gen:\n",
    "        if len(records) == 0:\n",
    "            break\n",
    "\n",
    "        for rec in records:\n",
    "            send_message(producer, YELLOW_TAXI_TOPIC, rec)\n",
    "\n",
    "        producer.flush()\n",
    "    print(\"All messages sent and flushed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf8628",
   "metadata": {},
   "source": [
    "# Consumer\n",
    "\n",
    "We need to define the conumer which is going to subscribe to the same topic as our producer. So as before, it is going to subscribe to the 'yellow-taxi' topic, on the local machine using the port 9092. It is going to start processing from the earliest available message.\n",
    "For the each recieved message we are going to log it to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_ID = 'my_confluent_consumer_group'\n",
    "BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "YELLOW_TAXI_TOPIC = 'yellow-taxi'\n",
    "\n",
    "def create_consumer():\n",
    "    conf = {\n",
    "        'bootstrap.servers': BOOTSTRAP_SERVERS,\n",
    "        'group.id': GROUP_ID,\n",
    "        'auto.offset.reset': 'earliest',\n",
    "        'enable.auto.commit': True,     \n",
    "        'auto.commit.interval.ms': 1000 \n",
    "    }\n",
    "    try:\n",
    "        consumer = Consumer(conf)\n",
    "        print(\"Confluent Kafka Consumer initialized successfully.\")\n",
    "        return consumer\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Confluent Kafka Consumer: {e}\")\n",
    "        return None\n",
    "\n",
    "def consume_messages(consumer):\n",
    "    consumer.subscribe([YELLOW_TAXI_TOPIC])\n",
    "    print(f\"Topic: {YELLOW_TAXI_TOPIC}, Group: {GROUP_ID}\")\n",
    "\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        else:\n",
    "            decoded_value = json.loads(msg.value().decode('utf-8'))\n",
    "            print(decoded_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70862b1",
   "metadata": {},
   "source": [
    "Here we define our consumer and use it to recieve the messages from the topic. In order to stop it we can terminate the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = create_consumer()\n",
    "if consumer:\n",
    "    consume_messages(consumer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8feda",
   "metadata": {},
   "source": [
    "### Running in paralle\n",
    "\n",
    "We can run consumer and producer in paralle if we run the 'producer.py' and 'consumer.py' from different terminal session. This way we can see the true power of Kafka messaging system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013b251",
   "metadata": {},
   "source": [
    "# Faust\n",
    "\n",
    "Here we define a consumer with Faust library. Same as before it is going to subscribe to the 'yellow-taxi' topic, on the local machine, on the port 9092. \n",
    "Because Boroughs are not in the provided data, we need to provide the mapping to the conumer function, so we can do our analysis. We load the mappings from the location_to_borough.csv file and use it later in the program. We chose some interesting locations for which we are going to do the anlysis, these locations include JFK Airport, Times Square, and some others. \n",
    "We need to define the class, whihc is going to represent the data that is present in the kafka topic. Whith this class defined, we can be assured that all the data that we recieve is going to follow this pattern. \n",
    "Also we define a StatsData class which we are going to be using to store our data in RockDb, in memory data base. This table is going to store the data for the last 360 seconds, so we can print out the nececary statistics. \n",
    "Whenever we recieve a new message we update the required data for the given borough, and for the location if it is among the ones we are interested in. \n",
    "Each 30 seconds we are going to print out required information for the last 60-second window to the user. \n",
    "\n",
    "We cannot run this application from the terminal so we provided the faust_consumer.py script which can be run to test its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faust\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "BOOTSTRAP_SERVERS = 'localhost:9092'\n",
    "YELLOW_TAXI_TOPIC = 'yellow-taxi'\n",
    "WINDOW_SIZE_SECONDS = 300  \n",
    "WINDOW_EXPIRES_SECONDS = 360\n",
    "\n",
    "LOCATION_TO_BOROUGH = pd.read_csv('location_to_borough.csv').set_index('LocationID')['Borough'].to_dict()\n",
    "\n",
    "CHOSEN_LOCATIONS = [\n",
    "    132,  # JFK Airport\n",
    "    138,  # LaGuardia Airport\n",
    "    230,  # Times Square\n",
    "    161,  # Midtown Center\n",
    "    237,  # Upper East Side South\n",
    "    79,   # East Harlem South\n",
    "    186,  # Penn Station / Madison Sq West\n",
    "    249,  # West Village\n",
    "    90,   # Flatiron\n",
    "    48,   # Clinton East\n",
    "]\n",
    "\n",
    "app = faust.App(\n",
    "    'taxi-stats-consumer',\n",
    "    broker=f'kafka://{BOOTSTRAP_SERVERS}',\n",
    "    value_serializer='json',\n",
    "    store='rocksdb://',  \n",
    ")\n",
    "\n",
    "\n",
    "class TaxiRide(faust.Record, serializer='json'):\n",
    "    pickup_datetime: str\n",
    "    dropoff_datetime: str\n",
    "    PULocationID: int\n",
    "    DOLocationID: int\n",
    "    distance: float\n",
    "    base_fare: float\n",
    "    passenger_count: int \n",
    "    payment_type: int\n",
    "\n",
    "class StatsData(faust.Record):\n",
    "    count: int = 0\n",
    "    distance_sum: float = 0.0\n",
    "    distance_sq_sum: float = 0.0\n",
    "    fare_sum: float = 0.0\n",
    "    fare_sq_sum: float = 0.0\n",
    "    passenger_sum: int = 0\n",
    "    passenger_sq_sum: int = 0\n",
    "\n",
    "\n",
    "taxi_topic = app.topic(YELLOW_TAXI_TOPIC, value_type=TaxiRide)\n",
    "\n",
    "# Tumbling windows: Non-overlapping, fixed-size windows.\n",
    "borough_stats_table = app.Table(\n",
    "    'borough_stats',\n",
    "    default=StatsData,\n",
    "    key_type=str,\n",
    "    value_type=StatsData,\n",
    "    partitions=1\n",
    ").tumbling(\n",
    "    timedelta(seconds=WINDOW_SIZE_SECONDS),\n",
    "    expires=timedelta(seconds=WINDOW_EXPIRES_SECONDS),\n",
    "    key_index=True \n",
    ")\n",
    "\n",
    "location_stats_table = app.Table(\n",
    "    'location_stats',\n",
    "    default=StatsData,\n",
    "    key_type=int,\n",
    "    value_type=StatsData,\n",
    "    partitions=1\n",
    ").tumbling(\n",
    "    timedelta(seconds=WINDOW_SIZE_SECONDS),\n",
    "    expires=timedelta(seconds=WINDOW_EXPIRES_SECONDS),\n",
    "    key_index=True\n",
    ")\n",
    "\n",
    "def calculate_and_format_stats(stats, name):\n",
    "    if stats.count == 0:\n",
    "        return f\"{name}: No data in current window.\"\n",
    "\n",
    "    mean_dist = stats.distance_sum / stats.count\n",
    "    mean_fare = stats.fare_sum / stats.count\n",
    "    mean_pass = stats.passenger_sum / stats.count\n",
    "\n",
    "    std_dist = math.sqrt(max(0, (stats.distance_sq_sum / stats.count) - (mean_dist ** 2)))\n",
    "    std_fare = math.sqrt(max(0, (stats.fare_sq_sum / stats.count) - (mean_fare ** 2)))\n",
    "    std_pass = math.sqrt(max(0, (stats.passenger_sq_sum / stats.count) - (mean_pass ** 2)))\n",
    "\n",
    "    return (\n",
    "        f\"--- {name} (Window) ---\\n\"\n",
    "        f\"  Count: {stats.count}\\n\"\n",
    "        f\"  Distance: Mean={mean_dist:.2f}, StdDev={std_dist:.2f}\\n\"\n",
    "        f\"  Fare:     Mean={mean_fare:.2f}, StdDev={std_fare:.2f}\\n\"\n",
    "        f\"  Passengers: Mean={mean_pass:.2f}, StdDev={std_pass:.2f}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "@app.agent(taxi_topic)\n",
    "async def process_taxi_ride(stream):\n",
    "    async for ride in stream:\n",
    "        location_id = int(ride.PULocationID)\n",
    "        borough = LOCATION_TO_BOROUGH.get(location_id, \"Unknown\")\n",
    "\n",
    "        current_borough_stats = borough_stats_table[borough].value()\n",
    "        current_borough_stats.count += 1\n",
    "        current_borough_stats.distance_sum += ride.distance\n",
    "        current_borough_stats.distance_sq_sum += ride.distance ** 2\n",
    "        current_borough_stats.fare_sum += ride.base_fare\n",
    "        current_borough_stats.fare_sq_sum += ride.base_fare ** 2\n",
    "        current_borough_stats.passenger_sum += ride.passenger_count\n",
    "        current_borough_stats.passenger_sq_sum += ride.passenger_count ** 2\n",
    "        borough_stats_table[borough] = current_borough_stats\n",
    "\n",
    "        if location_id in CHOSEN_LOCATIONS:\n",
    "            current_location_stats = location_stats_table[location_id].value()\n",
    "            current_location_stats.count += 1\n",
    "            current_location_stats.distance_sum += ride.distance\n",
    "            current_location_stats.distance_sq_sum += ride.distance ** 2\n",
    "            current_location_stats.fare_sum += ride.base_fare\n",
    "            current_location_stats.fare_sq_sum += ride.base_fare ** 2\n",
    "            current_location_stats.passenger_sum += ride.passenger_count\n",
    "            current_location_stats.passenger_sq_sum += ride.passenger_count ** 2\n",
    "            location_stats_table[location_id] = current_location_stats\n",
    "\n",
    "\n",
    "@app.timer(interval=30.0)  \n",
    "async def print_stats():\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(f\"ROLLING STATISTICS (Window: {WINDOW_SIZE_SECONDS}s)\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    print(\"\\n## Borough Statistics ##\")\n",
    "    known_boroughs = set(LOCATION_TO_BOROUGH.values())\n",
    "    for borough in known_boroughs:\n",
    "        stats = borough_stats_table[borough].now() \n",
    "        print(f\"{stats}\")\n",
    "        if stats.count > 0:\n",
    "            print(calculate_and_format_stats(stats, f\"Borough: {borough}\"))\n",
    "\n",
    "    print(\"\\n## Chosen Location Statistics ##\")\n",
    "    for loc_id in CHOSEN_LOCATIONS:\n",
    "        stats = location_stats_table[loc_id].now()\n",
    "        if stats.count > 0:\n",
    "            print(calculate_and_format_stats(stats, f\"Location ID: {loc_id}\"))\n",
    "\n",
    "    print(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
