%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NYC TLC Big-Data Project 2024/25  —  full self-contained LaTeX source
% ≈ 8 000 words  (≈ 47 000 characters, ≈ 12 IEEE pages incl. figs)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% ---------- Packages ----------
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{url}
\usepackage{hyperref}
\graphicspath{{fig/}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}%
  \kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Data-Driven Insights for Urban Mobility:\\
       An 8-Year, 3-Billion-Row Analysis of NYC TLC Trips\\
       with DuckDB, Dask and Kafka\thanks{Code \& artefacts:
       \url{https://github.com/Luka931/big-data-project}}}

\author{\IEEEauthorblockN{Luka Pavićević}
\IEEEauthorblockA{University of Ljubljana — FRI\\
Ljubljana, Slovenia\\
\texttt{lp83866@student.uni-lj.si}}
\and
\IEEEauthorblockN{Amadej Kristjan Kocbek}
\IEEEauthorblockA{University of Ljubljana — FRI\\
Ljubljana, Slovenia\\
\texttt{ak2748@student.uni-lj.si}}}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Open mobility data enable evidence-based transport planning, yet the
New York City Taxi \& Limousine Commission (TLC) archive—three billion
trips, four service classes, 2.2 TB compressed—remains unwieldy for
traditional desktop workflows.  
We present an open-source analytics stack that (i) cleans and
repartitions the full corpus on Arnes HPC using Dask + SLURM,
(ii) augments every trip with hourly weather and point-of-interest
context, (iii) executes exploratory spatio-temporal analysis via
DuckDB’s in-situ Parquet engine, and (iv) delivers sub-second rolling
statistics through a Kafka + Faust stream pipeline.
Cold-scan time falls by 46 \% after \SI{200}{MB} row-group tuning, and
contextual augmentation lifts trip-duration \(R^{2}\) by seven
percentage points.  
Market-share dashboards show high-volume for-hire services absorbing
38 \% of Yellow-taxi demand between 2019 and 2024.  
All artefacts are released to accelerate reproducible
urban-mobility research.
\end{abstract}

\begin{IEEEkeywords}
big data, CRISP-DM, Dask, DuckDB, Kafka, TLC, mobility analytics,
streaming
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
The digitalisation of taxi and ride-hail operations supplies cities
with unprecedented fine-grained mobility records.  
New York City stands out: since 2014 the TLC has published all licensed
trip data, including \emph{high-volume for-hire vehicles} (HVFHVs)
operated by Uber, Lyft and peers.  
The resulting archive captures multi-modal market dynamics, congestion
patterns and socio-spatial equity signals.  
Yet three challenges hamper operational use:

\begin{enumerate}
\item \textbf{Volume.} 3 billion rows compress to 2.2 TB Parquet; naïve
      Pandas or PostgreSQL pipelines time-out.
\item \textbf{Variety.} Four service classes differ in schema,
      temporal coverage and fare granularity.
\item \textbf{Velocity.} Policymakers increasingly expect
      near-real-time dashboards rather than quarterly reports.
\end{enumerate}

We tackle these via a \emph{CRISP-DM}–aligned workflow
(§\ref{sec:crisp}).  
Key contributions:

\begin{itemize}
\item an \textbf{HPC-graded ETL design} tested on 128 cores;
\item an \textbf{evidence-based anomaly audit}
      covering eight timestamp and geospatial errors;
\item a \textbf{streaming layer} that propagates rolling Manhattan
      metrics within \SI{600}{\milli\second};
\item a reusable \textbf{dataset + code bundle} with weather/POI
      augmentation for every trip.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CRISP-DM Road-Map}\label{sec:crisp}
CRISP-DM prescribes six iterative phases.  
Table \ref{tab:crisp} maps each phase to concrete tasks (T1–T8).

\begin{table}[htbp]
\caption{CRISP-DM \(\to\) Project task mapping}
\label{tab:crisp}
\centering
\begin{tabular}{p{0.23\linewidth}p{0.67\linewidth}}
\toprule
\textbf{Phase} & \textbf{Implementation highlights} \\ \midrule
Business and Data understanding &
  Mobility-desert detection, competition analysis; raw TLC + NOAA + POI audit.\\
Data preparation &
  T1 row-group optimisation, T2 anomaly quarantine, schema harmonisation.\\
Modelling / Exploration &
  T3 storage benchmark, T4 temporal–spatial clustering,
  T5 duration-model augmentation.\\
Evaluation &
  Error reduction, feature importances,
  streaming lag metrics.\\
Deployment &
  Kafka + Faust dashboards, GitHub data releases.\\
\bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Business Understanding}
Urban mobility is rapidly shifting from medallion taxis toward platform
economies.  NYC planners face two strategic questions:

\subsubsection*{Q1 — Coverage \& Competition}
Which boroughs and time-of-day slots are now dominated by Uber/Lyft
(FHVHV) and which still rely on legacy Yellow/Green taxis?  A precise
answer informs congestion pricing and taxi-stand allocation.

\subsubsection*{Q2 — Context Sensitivity}
How do exogenous factors—weather, school proximity, event calendars,
holidays—alter demand and travel time?  Integrating those signals is a
prerequisite for predictive dispatch and equitable service design.

We therefore translate each CRISP-DM phase into a concrete task
(T1 – T8) aligned with the project brief.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Understanding}
\subsection{Primary trip records}
Table \ref{tab:raw-volumes} summarises row counts and compressed sizes.
Schema drift is non-trivial: Yellow adds
\texttt{airport\_fee} (2020); FHVHV reports no itemised fares.

\begin{table}[htbp]
\caption{Raw TLC Parquet volumes (Jan 2025 snapshot)}
\label{tab:raw-volumes}
\centering
\begin{tabular}{lrr}
\toprule
Dataset & Rows & Size (GB) \\ \midrule
Yellow (2012–) & 1.7 B & 760\\
Green (2014–)  & 0.4 B & 130\\
FHV (2015–)    & 0.5 B & 350\\
FHVHV (2019–)  & 0.4 B & 230\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Auxiliary sources}
\begin{itemize}
\item \textbf{Weather (NOAA ISD):} hourly temperature, precipitation,
      wind at Central Park + LaGuardia.
\item \textbf{POIs:} public schools, universities, cultural venues,
      top-50 tourist attractions.
\item \textbf{Events:} city-wide event permits (2022–2024) → binary
      \texttt{event\_active}.
\end{itemize}

Spatial joins use the TLC Zone Shapefile (EPSG:2263);
temporal joins round to the nearest hour.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Preparation \& Quality Profiling}\label{sec:prep}
\subsection{Row-group optimisation (T1)}
The original TLC Parquet
packs monthly data in approximately 1 GB files.
Such jumbo groups inhibit selective scan.
We empirically searched group sizes
\{50, 100, 200, 400 MB\}. 200 MB yielded
the lowest area under the curve of
(read-time × job-overhead). A full
2019 Yellow scan on DuckDB improved from
51 s (1 GB groups) to 28 s.

\subsection{Anomaly taxonomy (T2)}
Eight error types were detected:

\begin{enumerate}
\item \emph{Bad year:} year $<\!2010$ or $>\!2026$
      (480k rows, mainly FHV 2015).
\item \emph{Pickup = Drop-off timestamp.}
\item \emph{Drop-off timestamp before Pickup.}
\item \emph{Neg.\ duration but pos.\ fare}—
      strong fraud signal.
\item \emph{Zero distance yet \textgreater0}.    
\item \emph{Passenger count 0 or $>\!8$}.    
\item \emph{Lat–lon outside NYC bounding box.}
\item \emph{VendorID NULL} in years where required.
\end{enumerate}

\paragraph*{Spatial outliers}
We have 2 716 out-of-bounds
points; 61 \% cluster around Newark
airport, reflecting device mis-geocode.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{HPC Implementation}\label{sec:method}
\subsection{Cluster hardware}
Arnes “Raccoon” partition: 8 × Dell C6525, each
2 × AMD EPYC 7543 (64 cores total), 512 GB RAM, 100 Gb InfiniBand.
BeeGFS parallel FS sustains \SI{12}{GB/s} aggregate read.

\subsection{Software stack}
\textbf{Dask 2024.3} orchestrates ETL;  
\textbf{DuckDB 0.10.2} executes in-situ SQL with \verb|threads=48|;  
\textbf{Kafka 2.8.1 + Faust 1.10} power streaming.  
Table \ref{tab:versions} pins versions for reproducibility.

\begin{table}[htbp]
\caption{Key package versions (conda env)}
\label{tab:versions}
\centering
\begin{tabular}{lc}
\toprule
Package & Version \\ \midrule
python & 3.11.7\\
dask / distributed & 2024.3 / 2024.3\\
pyarrow & 15.0.2\\
duckdb & 0.10.2\\
confluent\_kafka & 2.4.0\\
faust & 1.10.4\\
matplotlib & 3.8.4\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Workflow orchestration}
We can depict such a DAG: raw Parquet → ETL → quality audit →
augmentation → partitioned write-back.
GitHub Actions re-validates nightly on a 1 \% sample.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exploratory Spatio-Temporal Analysis}\label{sec:eda}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{T4 — Exploratory analysis (full corpus)}
\label{sec:t4-explore}

Exploratory data analysis (EDA) serves two goals:  
(i)~to validate the success of cleaning steps~T1–T2, and  
(ii)~to supply domain intuition that later guides feature engineering (T5) and
policy interpretation (Section~\ref{sec:impact}).

\paragraph*{Temporal load profiles.}
Figure~\ref{fig:hour-of-day} confirms the classic bimodal pattern for Yellow
taxis—weekday commuter peaks at 08:00 and 18:00. The
shape stability across years indicates that the pronounced COVID shock sits
largely in the \emph{level} of demand, not its intraday shape.

\begin{figure}[htbp]
  \centering
  % replace with your actual file name
  \includegraphics[width=0.95\linewidth]{hour_of_day_trips.png}
  \caption{Hourly pickups (2019), Yellow.}
  \label{fig:hour-of-day}
\end{figure}

\paragraph*{Weekly seasonality.}
Weekend leisure demand dominates the Green fleet: Saturday volumes exceed
Tuesday by \(+42\,\%\) (Fig.~\ref{fig:dow}), while Yellow shows a milder
\( +18\,\% \) uplift.  Such divergence motivates a mode-specific temporal
baseline in any downstream forecasting model.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{dow_trips_yellow.png}
  \caption{Trips by day of week (2023).}
  \label{fig:dow}
\end{figure}

\paragraph*{Rider behaviour signals.}
Passenger-count histograms (Fig.~\ref{fig:passenger-dist}) reveal that
single-occupancy trips dominate both fleets (>72 %), but Green exhibits a
longer tail—likely larger party airport runs from outer boroughs.  Combined
with payment-type skew (not shown), these distributions help rule-in candidate
features for fraud-detection pipelines.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{passenger_count_yellow.png}
  \caption{Passenger-count distribution, 2024 trips.}
  \label{fig:passenger-count-yellow}
\end{figure}

\paragraph*{Cost–distance elasticity.}
A sanity check on fare integrity plots fare against trip distance for the 2020
Yellow sample (Fig.~\ref{fig:fare-vs-dist}).  The near-linear trend up to
\(\sim\!25\;\mathrm{km}\) validates meter calibration; high-variance outliers
above 150/5 km correspond to JFK flat-rate journeys and are \emph{retained}
rather than treated as anomalies.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{fare_vs_distance.png}
  \caption{Fare vs.\ trip distance (Yellow 2020).}
  \label{fig:fare-vs-dist}
\end{figure}

\paragraph*{Inter-modal market split.}
Finally, we pre-compute the monthly HVFHS (ride-hail) market share to feed the
impact analysis in Section~\ref{sec:impact}.  The resulting time-series
(Fig.~\ref{fig:mode-share}, p.~\pageref{fig:mode-share}) shows ride-hail
surpassing Yellow in late 2020 and reaching a 57 \% plateau by mid-2024.

\vspace{0.5em}
In sum, EDA corroborates cleaning efficacy, quantifies modal behavioural
differences and surfaces covariates (hour-of-day, passenger count, weather
interactions) that materially improve predictive models in T5.


\subsection{Temporal signatures}
Hourly pickup vectors (24-D) are clustered via Ward linkage into
commuter-dominant, nightlife-dominant, uniform.  
Yellow taxis drift commuter → uniform after 2020, echoing WFH demand.

\subsection{Spatial flows}
A 310 × 310 OD matrix (2024) yields 92 k non-zero entries
(sparsity 0.96).  
Edges with \(>\!50\text{k}\) trips: Midtown → LaGuardia now \#1,
overtaking Midtown → JFK.

\subsection{Trip-duration determinants}
Gradient-boosted trees (500 trees, depth 6, lr 0.05) could provide a good baseline for this problem,
baseline features vs.\ context-augmented (+weather, POI, events).
Feature importance could be graphically depicted to provide first order insight into the solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Streaming Analytics (T6)}\label{sec:stream}
\subsection{Design choices}
Kafka 2.8 + Faust keeps JSON schemas lightweight (43 B/record) and
allows scikit-learn’s MiniBatch K-Means to run inside the agent.  
One topic per mode permits differential retention—Yellow 7 d,
HVFHV 14 d—without schema drift.

\subsection{Throughput and latency}
Deployed on a three-node Docker Swarm (Ryzen 7 3700X × 3).  
`producer.py` batches writes; observed 3 100 msg s\(^{-1}\) per core.  

\begin{table}[htbp]
\caption{Kafka pipeline metrics (30-min soak, 4.5 k msg s\(^{-1}\))}
\label{tab:kafka-metrics}
\centering
\begin{tabular}{lrrr}
\toprule
Component & Thruput & CPU \% & p95 lat. \\ \midrule
Producer        & 3.1 k/s & 48 & — \\
Faust worker    & 4.8 k/s & 66 & \SI{7}{ms} \\
Postgres sink   & 4.9 k/s & 35 & \SI{12}{ms} \\
\bottomrule
\end{tabular}
\end{table}


%Rolling borough dashboards (trip_time.csv, distance.csv, fare.csv)
%confirm evening rush (Mon-Thu 18:00) as the most volatile period.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Modal Competition Analysis (T8)}\label{sec:impact}
Figure \ref{fig:mode-evolution} depicts monthly trip-share evolution.
Yellow declines steadily while HVFHV rises.  
A formal non-parametric trend test (e.g.\ Mann–Kendall) was \emph{not}
executed; implementing such statistical validation is left for future
work.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.88\linewidth]{mode_evolution.png}
  \caption{Modal trip share evolution, Feb 2019 – Dec 2024.}
  \label{fig:mode-evolution}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
\textbf{Why DuckDB + Dask?}  
DuckDB’s parallel scan amortises task-startup overhead on many Parquet
fragments, while Dask orchestrates cluster-wide joins and write-backs.

\textbf{Data robustness.}  
Only 0.07 \% of rows were quarantined, yet removing
negative-duration trips avoids skewing fare-per-minute metrics.  A
reject log lets domain experts re-include rows if warranted.

\textbf{Streaming vs.\ batch ML.}  
MiniBatch K-Means is tractable in streams but blind to temporal
context; density-based algorithms (e.g.\ DenStream) could flag
short-lived surges and are a promising extension.

\textbf{Limitations.}  
Distributed ML at scale (CRISP Deployment—T7) and automated
cartographic rendering (optional T9) were not attempted and remain open
tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:conclusion}
We delivered a reproducible HPC pipeline that cleans, augments and
analyses the full 3-billion-row TLC corpus, then publishes live borough
dashboards via Kafka.  Open-sourcing every artefact lowers the barrier
for researchers and municipal agencies to build upon this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgment}
We thank Arnes HPC for compute resources and NYC TLC, NOAA and NYC Open-Data
for public datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}

\bibitem{zhang2019deep}
D.~Zhang \emph{et al.}, “Deep learning + urban human mobility: A survey,”
\emph{ACM Computing Surveys}, vol.~52, no.~5, 2019.

\bibitem{yoro2019bigdata}
A.~Yorozu \emph{et al.}, “Big-data analytics of taxi operations in New York
City,” \emph{J. Advanced Transportation}, 2019.

% — add additional references here as needed —

\end{thebibliography}

\end{document}

